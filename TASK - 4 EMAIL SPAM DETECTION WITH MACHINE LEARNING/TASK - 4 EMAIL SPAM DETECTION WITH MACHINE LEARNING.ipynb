{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea475c55",
   "metadata": {},
   "source": [
    "# EMAIL SPAM DETECTION WITH MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9938d",
   "metadata": {},
   "source": [
    "In this project, we develop a Python-based email spam detector using machine learning to classify emails as either spam or non-spam, addressing the issue of unwanted and potentially harmful email communications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30b9cb",
   "metadata": {},
   "source": [
    "Required Modules:\n",
    "\n",
    "Pandas: For data manipulation.\n",
    "NumPy: Fundamental package for scientific computing.\n",
    "Matplotlib: For creating quality plots.\n",
    "Seaborn: Data visualization library based on Matplotlib.\n",
    "SciPy: Ecosystem for mathematics, science, and engineering software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f326f178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prajakta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Prajakta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Prajakta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f27fb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading of csv file\n",
    "df = pd.read_csv('spam.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "158f4236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email spam detection dataset is: \n",
      "         v1                                                 v2 Unnamed: 2  \\\n",
      "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "...    ...                                                ...        ...   \n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
      "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
      "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
      "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
      "5571   ham                         Rofl. Its true to its name        NaN   \n",
      "\n",
      "     Unnamed: 3 Unnamed: 4  \n",
      "0           NaN        NaN  \n",
      "1           NaN        NaN  \n",
      "2           NaN        NaN  \n",
      "3           NaN        NaN  \n",
      "4           NaN        NaN  \n",
      "...         ...        ...  \n",
      "5567        NaN        NaN  \n",
      "5568        NaN        NaN  \n",
      "5569        NaN        NaN  \n",
      "5570        NaN        NaN  \n",
      "5571        NaN        NaN  \n",
      "\n",
      "[5572 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Displaying dataset\n",
    "print(\"Email spam detection dataset is: \\n\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e319c760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows of the dataset are: \n",
      "      v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "\n",
      "Bottom 5 rows of the dataset are: \n",
      "         v1                                                 v2 Unnamed: 2  \\\n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
      "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
      "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
      "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
      "5571   ham                         Rofl. Its true to its name        NaN   \n",
      "\n",
      "     Unnamed: 3 Unnamed: 4  \n",
      "5567        NaN        NaN  \n",
      "5568        NaN        NaN  \n",
      "5569        NaN        NaN  \n",
      "5570        NaN        NaN  \n",
      "5571        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "#Top and Botton 5 rows of car price prediction dataset\n",
    "print(\"Top 5 rows of the dataset are: \\n\",df.head())\n",
    "print(\"\\n\\nBottom 5 rows of the dataset are: \\n\",df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c5d57",
   "metadata": {},
   "source": [
    "The Porter stemming algorithm, commonly referred to as the 'Porter stemmer,' is a linguistic process designed to eliminate common morphological and inflectional endings from English words. Its primary purpose lies in term normalization, a crucial step often employed in the establishment of Information Retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae884da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "lemmatize=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "for i in range(0,len(df)):\n",
    "  review=re.sub('[^a-zA-Z]', ' ', df['v2'][i])\n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "    \n",
    "  review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66fd3d",
   "metadata": {},
   "source": [
    "CountVectorizer is a valuable tool available in the Python scikit-learn library. It serves the purpose of converting textual data into a numerical vector representation by considering the frequency (count) of each word within the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6023c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y=pd.get_dummies(df['v1'])\n",
    "y=y.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d80d6",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a3a6c",
   "metadata": {},
   "source": [
    "Utilizing train_test_split(), you must supply the sequences you intend to divide, along with any optional parameters. This function returns a list of NumPy arrays, other sequences, or SciPy sparse matrices if applicable. The \"arrays\" parameter refers to the sequence of lists, NumPy arrays, pandas DataFrames, or similar array-like objects that contain the data you wish to partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0efe08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5148d9",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier employs Bayes' Theorem to categorize data into distinct classes, assuming that all predictors are independent of each other. It operates under the assumption that a particular feature in a class is unrelated to the presence of other features.\n",
    "\n",
    "The multinomial Naive Bayes classifier is specifically designed for classification tasks involving discrete features, such as word counts for text classification. Typically, the multinomial distribution expects integer feature counts, although fractional counts like tf-idf can also be used effectively in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d8b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb9e43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=spam_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1f3c6",
   "metadata": {},
   "source": [
    "**Model Precision:\n",
    "\n",
    "The model precision score measures the proportion of positively predicted labels that are actually correct. Precision is also known as the positive predictive value. It is a useful metric for evaluating the accuracy of a model's positive predictions and is sensitive to the class distribution.\n",
    "\n",
    "**Accuracy Score:\n",
    "\n",
    "Model recall score represents the model’s ability to correctly predict the positives out of actual positives. This is unlike precision, which measures how many predictions made by models are actually positive out of all positive predictions made. Recall Score is another important metric for assessing the performance of a classification model.\n",
    "\n",
    "**Confusion Matrix:\n",
    "\n",
    "A confusion matrix, also referred to as an error matrix, is a tool that helps assess and predict the validity of a classification model. It provides valuable insights into different types of errors the model may make, such as false positives and false negatives.\n",
    "\n",
    "**Classification Report:\n",
    "\n",
    "A classification report is a comprehensive performance evaluation metric in machine learning. It provides a summary of key metrics like precision, recall, F1 Score, and support for each class in your trained classification model. This report is essential for a detailed understanding of a model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98f4e18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[943   6]\n",
      " [  9 157]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dacd3f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.9865470852017937\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dd6d301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       949\n",
      "           1       0.96      0.95      0.95       166\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.97      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report: {}\".format(classification_report(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc39e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
